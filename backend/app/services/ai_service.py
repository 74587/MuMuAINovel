"""AIæœåŠ¡å°è£… - ç»Ÿä¸€çš„OpenAIå’ŒClaudeæ¥å£"""
from typing import Optional, AsyncGenerator, List, Dict, Any
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from app.config import settings as app_settings
from app.logger import get_logger
import httpx

logger = get_logger(__name__)


class AIService:
    """AIæœåŠ¡ç»Ÿä¸€æ¥å£ - æ”¯æŒä»ç”¨æˆ·è®¾ç½®æˆ–å…¨å±€é…ç½®åˆå§‹åŒ–"""
    
    def __init__(
        self,
        api_provider: Optional[str] = None,
        api_key: Optional[str] = None,
        api_base_url: Optional[str] = None,
        default_model: Optional[str] = None,
        default_temperature: Optional[float] = None,
        default_max_tokens: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆä¼˜åŒ–å¹¶å‘æ€§èƒ½ï¼‰
        
        Args:
            api_provider: APIæä¾›å•† (openai/anthropic)ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            api_key: APIå¯†é’¥ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            api_base_url: APIåŸºç¡€URLï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_model: é»˜è®¤æ¨¡å‹ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_temperature: é»˜è®¤æ¸©åº¦ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_max_tokens: é»˜è®¤æœ€å¤§tokensï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
        """
        # ä¿å­˜ç”¨æˆ·è®¾ç½®æˆ–ä½¿ç”¨å…¨å±€é…ç½®
        self.api_provider = api_provider or app_settings.default_ai_provider
        self.default_model = default_model or app_settings.default_model
        self.default_temperature = default_temperature or app_settings.default_temperature
        self.default_max_tokens = default_max_tokens or app_settings.default_max_tokens
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        openai_key = api_key if api_provider == "openai" else app_settings.openai_api_key
        if openai_key:
            try:
                limits = httpx.Limits(
                    max_keepalive_connections=50,
                    max_connections=100,
                    keepalive_expiry=30.0
                )
                
                http_client = httpx.AsyncClient(
                    timeout=httpx.Timeout(connect=60.0, read=180.0, write=60.0, pool=60.0),
                    limits=limits,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                )
                
                client_kwargs = {
                    "api_key": openai_key,
                    "http_client": http_client
                }
                
                base_url = api_base_url if api_provider == "openai" else app_settings.openai_base_url
                if base_url:
                    client_kwargs["base_url"] = base_url
                
                self.openai_client = AsyncOpenAI(**client_kwargs)
                self.openai_http_client = http_client
                self.openai_api_key = openai_key
                self.openai_base_url = base_url
                logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.openai_client = None
                self.openai_http_client = None
                self.openai_api_key = None
                self.openai_base_url = None
        else:
            self.openai_client = None
            self.openai_http_client = None
            self.openai_api_key = None
            self.openai_base_url = None
            logger.warning("OpenAI API keyæœªé…ç½®")
        
        # åˆå§‹åŒ–Anthropicå®¢æˆ·ç«¯
        anthropic_key = api_key if api_provider == "anthropic" else app_settings.anthropic_api_key
        if anthropic_key:
            try:
                limits = httpx.Limits(
                    max_keepalive_connections=50,
                    max_connections=100,
                    keepalive_expiry=30.0
                )
                
                http_client = httpx.AsyncClient(
                    timeout=httpx.Timeout(connect=60.0, read=180.0, write=60.0, pool=60.0),
                    limits=limits,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                )
                
                client_kwargs = {
                    "api_key": anthropic_key,
                    "http_client": http_client
                }
                
                base_url = api_base_url if api_provider == "anthropic" else app_settings.anthropic_base_url
                if base_url:
                    client_kwargs["base_url"] = base_url
                
                self.anthropic_client = AsyncAnthropic(**client_kwargs)
                logger.info("âœ… Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.anthropic_client = None
        else:
            self.anthropic_client = None
            logger.warning("Anthropic API keyæœªé…ç½®")
    
    async def generate_text(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•† (openai/anthropic)
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        provider = provider or self.api_provider
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        if provider == "openai":
            return await self._generate_openai(
                prompt, model, temperature, max_tokens, system_prompt
            )
        elif provider == "anthropic":
            return await self._generate_anthropic(
                prompt, model, temperature, max_tokens, system_prompt
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
    
    async def generate_text_stream(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•†
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        provider = provider or self.api_provider
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        if provider == "openai":
            async for chunk in self._generate_openai_stream(
                prompt, model, temperature, max_tokens, system_prompt
            ):
                yield chunk
        elif provider == "anthropic":
            async for chunk in self._generate_anthropic_stream(
                prompt, model, temperature, max_tokens, system_prompt
            ):
                yield chunk
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
    
    async def _generate_openai(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆæ–‡æœ¬"""
        if not self.openai_http_client:
            raise ValueError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨OpenAI APIï¼ˆç›´æ¥HTTPè¯·æ±‚ï¼‰")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - æ¸©åº¦: {temperature}")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            url = f"{self.openai_base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            logger.debug(f"  - è¯·æ±‚URL: {url}")
            logger.debug(f"  - è¯·æ±‚å¤´: Authorization=Bearer ***")
            
            response = await self.openai_http_client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            
            data = response.json()
            
            logger.info(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")
            logger.info(f"  - å“åº”ID: {data.get('id', 'N/A')}")
            logger.info(f"  - é€‰é¡¹æ•°é‡: {len(data.get('choices', []))}")
            
            if not data.get('choices'):
                logger.error("âŒ OpenAIè¿”å›çš„choicesä¸ºç©º")
                raise ValueError("APIè¿”å›çš„å“åº”æ ¼å¼é”™è¯¯ï¼šchoiceså­—æ®µä¸ºç©º")
            
            choice = data['choices'][0]
            message = choice.get('message', {})
            finish_reason = choice.get('finish_reason')
            
            # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåªä½¿ç”¨contentï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå¿½ç•¥reasoning_contentï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
            # reasoning_contentæ˜¯AIçš„æ€è€ƒè¿‡ç¨‹ï¼Œä¸æ˜¯æˆ‘ä»¬éœ€è¦çš„JSONç»“æœ
            content = message.get('content', '')
            
            # æ£€æŸ¥æ˜¯å¦å› è¾¾åˆ°é•¿åº¦é™åˆ¶è€Œæˆªæ–­
            if finish_reason == 'length':
                logger.warning(f"âš ï¸  å“åº”å› è¾¾åˆ°max_tokensé™åˆ¶è€Œè¢«æˆªæ–­")
                logger.warning(f"  - å½“å‰max_tokens: {max_tokens}")
                logger.warning(f"  - å»ºè®®: å¢åŠ max_tokenså‚æ•°ï¼ˆæ¨è2000+ï¼‰")
            
            if content:
                logger.info(f"  - è¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.info(f"  - å®ŒæˆåŸå› : {finish_reason}")
                logger.info(f"  - è¿”å›å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰: {content[:200]}")
                return content
            else:
                logger.error("âŒ AIè¿”å›äº†ç©ºå†…å®¹")
                logger.error(f"  - å®Œæ•´å“åº”: {data}")
                logger.error(f"  - å®ŒæˆåŸå› : {finish_reason}")
                
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                if finish_reason == 'length':
                    raise ValueError(f"AIå“åº”è¢«æˆªæ–­ä¸”æ— æœ‰æ•ˆå†…å®¹ã€‚è¯·å¢åŠ max_tokenså‚æ•°ï¼ˆå½“å‰: {max_tokens}ï¼Œå»ºè®®: 2000+ï¼‰")
                else:
                    raise ValueError(f"AIè¿”å›äº†ç©ºå†…å®¹ï¼ˆfinish_reason: {finish_reason}ï¼‰ï¼Œè¯·æ£€æŸ¥APIé…ç½®æˆ–ç¨åé‡è¯•")
            
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥ (HTTP {e.response.status_code})")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {e.response.text}")
            logger.error(f"  - æ¨¡å‹: {model}")
            raise Exception(f"APIè¿”å›é”™è¯¯ ({e.response.status_code}): {e.response.text}")
        except Exception as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"  - æ¨¡å‹: {model}")
            raise
    
    async def _generate_openai_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨OpenAIæµå¼ç”Ÿæˆæ–‡æœ¬"""
        if not self.openai_http_client:
            raise ValueError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨OpenAIæµå¼APIï¼ˆç›´æ¥HTTPè¯·æ±‚ï¼‰")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            
            url = f"{self.openai_base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True
            }
            
            async with self.openai_http_client.stream('POST', url, headers=headers, json=payload) as response:
                response.raise_for_status()
                logger.info(f"âœ… OpenAIæµå¼APIè¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
                
                chunk_count = 0
                has_content = False
                finish_reason = None
                
                async for line in response.aiter_lines():
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        
                        try:
                            import json
                            data = json.loads(data_str)
                            if 'choices' in data and len(data['choices']) > 0:
                                choice = data['choices'][0]
                                delta = choice.get('delta', {})
                                finish_reason = choice.get('finish_reason') or finish_reason
                                
                                # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåªæ”¶é›†contentï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå¿½ç•¥reasoning_contentï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
                                # reasoning_contentæ˜¯AIçš„æ€è€ƒè¿‡ç¨‹ï¼Œä¸æ˜¯æˆ‘ä»¬éœ€è¦çš„JSONç»“æœ
                                content = delta.get('content', '')
                                
                                if content:
                                    chunk_count += 1
                                    has_content = True
                                    yield content
                        except json.JSONDecodeError:
                            continue
                
                # æ£€æŸ¥æ˜¯å¦å› é•¿åº¦é™åˆ¶æˆªæ–­
                if finish_reason == 'length':
                    logger.warning(f"âš ï¸  æµå¼å“åº”å› è¾¾åˆ°max_tokensé™åˆ¶è€Œè¢«æˆªæ–­")
                    logger.warning(f"  - å½“å‰max_tokens: {max_tokens}")
                    logger.warning(f"  - å»ºè®®: å¢åŠ max_tokenså‚æ•°ï¼ˆæ¨è2000+ï¼‰")
                
                if not has_content:
                    logger.warning(f"âš ï¸  æµå¼å“åº”æœªè¿”å›ä»»ä½•å†…å®¹")
                    logger.warning(f"  - å®ŒæˆåŸå› : {finish_reason}")
                
                logger.info(f"âœ… OpenAIæµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªchunkï¼Œå®ŒæˆåŸå› : {finish_reason}")
            
        except httpx.TimeoutException as e:
            logger.error(f"âŒ OpenAIæµå¼APIè¶…æ—¶")
            logger.error(f"  - é”™è¯¯: {str(e)}")
            logger.error(f"  - æç¤º: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è€ƒè™‘ç¼©çŸ­prompté•¿åº¦")
            raise TimeoutError(f"AIæœåŠ¡è¶…æ—¶ï¼ˆ180ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦") from e
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ OpenAIæµå¼APIè°ƒç”¨å¤±è´¥ (HTTP {e.response.status_code})")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {await e.response.aread()}")
            raise
        except Exception as e:
            logger.error(f"âŒ OpenAIæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise
    
    async def _generate_anthropic(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> str:
        """ä½¿ç”¨Anthropicç”Ÿæˆæ–‡æœ¬"""
        if not self.anthropic_client:
            raise ValueError("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        try:
            response = await self.anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        except Exception as e:
            logger.error(f"Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def _generate_anthropic_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Anthropicæµå¼ç”Ÿæˆæ–‡æœ¬"""
        if not self.anthropic_client:
            raise ValueError("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨Anthropicæµå¼API")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            
            async with self.anthropic_client.messages.stream(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=[{"role": "user", "content": prompt}]
            ) as stream:
                logger.info(f"âœ… Anthropicæµå¼APIè¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
                
                chunk_count = 0
                async for text in stream.text_stream:
                    chunk_count += 1
                    yield text
                
                logger.info(f"âœ… Anthropicæµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªchunk")
                
        except httpx.TimeoutException as e:
            logger.error(f"âŒ Anthropicæµå¼APIè¶…æ—¶")
            logger.error(f"  - é”™è¯¯: {str(e)}")
            raise TimeoutError(f"AIæœåŠ¡è¶…æ—¶ï¼ˆ180ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦") from e
        except Exception as e:
            logger.error(f"âŒ Anthropicæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise


# åˆ›å»ºå…¨å±€AIæœåŠ¡å®ä¾‹
ai_service = AIService()


def create_user_ai_service(
    api_provider: str,
    api_key: str,
    api_base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int
) -> AIService:
    """
    æ ¹æ®ç”¨æˆ·è®¾ç½®åˆ›å»ºAIæœåŠ¡å®ä¾‹
    
    Args:
        api_provider: APIæä¾›å•†
        api_key: APIå¯†é’¥
        api_base_url: APIåŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokens
        
    Returns:
        AIServiceå®ä¾‹
    """
    return AIService(
        api_provider=api_provider,
        api_key=api_key,
        api_base_url=api_base_url,
        default_model=model_name,
        default_temperature=temperature,
        default_max_tokens=max_tokens
    )